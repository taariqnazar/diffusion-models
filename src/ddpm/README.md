# Denoising Diffusion Probabilistic Models: A Comprehensive Implementation and Analysis Report for CIFAR-10

**Report has been generated by GEMINI.**

## Introduction to Generative Modeling and Diffusion

The domain of generative deep learning has historically been defined by a triad of dominant architectures: Generative Adversarial Networks (GANs), Variational Autoencoders (VAEs), and Flow-based models. Each of these paradigms offers distinct advantages but also suffers from significant limitations.

GANs, while capable of synthesizing high-fidelity images, are notoriously difficult to train due to the adversarial minimax game, often leading to mode collapse and training instability. VAEs provide a principled probabilistic framework and stable training but frequently produce blurry samples due to the nature of the evidence lower bound (ELBO) and pixel-wise reconstruction objectives.

Denoising Diffusion Probabilistic Models (DDPMs) have emerged as a powerful alternative, fundamentally reshaping the state-of-the-art in image synthesis. Unlike their predecessors, diffusion models are grounded in non-equilibrium thermodynamics. The core intuition is to slowly destroy the structure in a data distribution through an iterative forward process of adding noise, and then learn a reverse process to restore the structure. This paradigm shift avoids the adversarial instability of GANs and the blurriness of VAEs, enabling the generation of high-fidelity, diverse samples that faithfully capture the underlying data distribution.

This report provides an exhaustive, expert-level guide to implementing and analyzing DDPMs. The focus is specifically on the CIFAR-10 dataset, a standard benchmark in computer vision comprising 60,000 $32 \times 32$ pixel color images across 10 classes. While computationally lighter than ImageNet, CIFAR-10 presents sufficient complexity to rigorously test the generative capabilities of the model, requiring the capture of diverse semantic categories—from biological entities like frogs and birds to mechanical objects like airplanes and trucks—within a low-resolution grid.

The choice of framework for this research is PyTorch. As indicated by current industry trends and research output, PyTorch remains the dominant platform for generative AI research due to its dynamic computation graph, Pythonic debugging experience, and extensive ecosystem of pre-built modules for diffusion (e.g., `diffusers`, `torch-fidelity`). While TensorFlow offers robust deployment pipelines, the research-centric nature of diffusion model development favors the flexibility of PyTorch.

---

## Theoretical Foundations of Diffusion Models

To implement a DDPM effectively, one must possess a deep understanding of the mathematical principles governing the forward and reverse diffusion processes. These processes define how information is systematically degraded and subsequently reconstructed.

### 2.1 The Forward Diffusion Process

The forward diffusion process, denoted as $q$, is a fixed Markov chain that gradually adds Gaussian noise to the data. Given a data distribution $x_0 \sim q(x_0)$, the process generates a sequence of latent variables $x_1, \dots, x_T$ of the same dimensionality as $x_0$.

The transition probability at each step $t$ is defined as a Gaussian distribution:

$$
q(x_t \mid x_{t-1}) = \mathcal{N}!\left(x_t; \sqrt{1-\beta_t},x_{t-1}, \beta_t \mathbf{I}\right).
$$

Here, $\beta_t$ represents the variance schedule, a hyperparameter that controls the amount of noise added at each step. As $t$ increases, the data $x_0$ is progressively corrupted. The parameters are chosen such that as $T \to \infty$, the distribution $x_T$ converges to an isotropic Gaussian $\mathcal{N}(0, \mathbf{I})$. This ensures that the reverse process can begin from a tractable prior distribution.

A critical property of this Gaussian diffusion formulation is the ability to sample $x_t$ at any arbitrary timestep $t$ directly from $x_0$, without iterating through all intermediate steps $x_1, \dots, x_{t-1}$. By defining $\alpha_t = 1 - \beta_t$ and $\bar{\alpha}*t = \prod*{s=1}^t \alpha_s$, the marginal distribution $q(x_t \mid x_0)$ can be derived as:

$$
q(x_t \mid x_0) = \mathcal{N}!\left(x_t; \sqrt{\bar{\alpha}_t},x_0, (1-\bar{\alpha}_t)\mathbf{I}\right).
$$

Using the reparameterization trick, this allows us to express $x_t$ as:

$$
x_t = \sqrt{\bar{\alpha}_t},x_0 + \sqrt{1-\bar{\alpha}_t},\epsilon,
$$

where $\epsilon \sim \mathcal{N}(0, \mathbf{I})$. This closed-form sampling is computationally efficient and central to the training loop, allowing the model to be trained on random timesteps in parallel batches.

### 2.2 The Reverse Denoising Process

The generative capability of the model lies in the reverse process, $p_\theta(x_{t-1} \mid x_t)$, which aims to invert the noise addition. Since the exact reverse posterior $q(x_{t-1} \mid x_t)$ involves the intractable data distribution $q(x_0)$, we approximate it using a neural network parameterized by $\theta$.

For small $\beta_t$, the reverse transition is also approximately Gaussian:

$$
p_\theta(x_{t-1} \mid x_t) = \mathcal{N}!\left(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)\right).
$$

In the canonical DDPM implementation by Ho et al. (2020), the variance $\Sigma_\theta(x_t, t)$ is kept fixed to $\sigma_t^2 \mathbf{I}$, where $\sigma_t^2$ is typically set to $\beta_t$ or:

$$
\tilde{\beta}*t = \frac{1-\bar{\alpha}*{t-1}}{1-\bar{\alpha}_t},\beta_t.
$$

The primary learning task is therefore to predict the mean $\mu_\theta(x_t, t)$.

### 2.3 Optimization Objective: From ELBO to Noise Prediction

The training objective is derived from maximizing the log-likelihood of the data, which is mathematically equivalent to minimizing the variational upper bound on the negative log-likelihood:

$$
\mathbb{E}*{q}!\left[-\log p*\theta(x_0)\right] \le L_{\text{VLB}}.
$$

The loss $L_{\text{VLB}}$ can be decomposed into a sum of KL divergence terms between the forward process posteriors and the learned reverse conditionals. Ho et al. simplified the parameterization of the mean $\mu_\theta$. They showed that predicting the mean of the distribution is equivalent to predicting the noise $\epsilon$ that was added to $x_0$ to generate $x_t$.

The relationship between the mean and the noise is given by:

$$
\mu_\theta(x_t, t) =
\frac{1}{\sqrt{\alpha_t}}
\left(
x_t - \frac{\beta_t}{\sqrt{1-\bar{\alpha}*t}},\epsilon*\theta(x_t, t)
\right),
$$

where $\epsilon_\theta(x_t, t)$ is a neural network that takes the noisy image $x_t$ and timestep $t$ as input and outputs the estimated noise.

Ignoring timestep-dependent weights yields the simplified loss used in practice:

$$
L_{\text{simple}}(\theta) =
\mathbb{E}*{t, x_0, \epsilon}
\left[
\left|
\epsilon - \epsilon*\theta!\left(\sqrt{\bar{\alpha}_t},x_0 + \sqrt{1-\bar{\alpha}_t},\epsilon, t\right)
\right|^2
\right].
$$

This objective is essentially a (weighted) Mean Squared Error (MSE) between the true noise $\epsilon$ and the predicted noise $\epsilon_\theta$. Empirically, this simplified objective often produces higher quality samples than the exact ELBO.

### 2.4 Variance Scheduling

The performance of the diffusion model is highly sensitive to the variance schedule $\beta_t$. For CIFAR-10, the standard linear schedule proposed by Ho et al. (2020) is widely adopted.

**Table 1: Variance Schedule Hyperparameters for CIFAR-10**

| Hyperparameter         |     Value | Description                                     |
| ---------------------- | --------: | ----------------------------------------------- |
| Schedule Type          |    Linear | $\beta_t$ increases linearly from start to end. |
| $\beta_{\text{start}}$ | $10^{-4}$ | Initial noise variance at $t=1$.                |
| $\beta_{\text{end}}$   |    $0.02$ | Final noise variance at $t=T$.                  |
| Timesteps ($T$)        |    $1000$ | Total number of diffusion steps.                |

This schedule ensures that at $t=T$, the cumulative product $\bar{\alpha}_T$ is sufficiently close to $0$, making the forward process output nearly a standard Gaussian.

---

## 3. Architectural Implementation: The Conditional U-Net

The neural network $\epsilon_\theta(x_t, t)$ must map a tensor of shape $(B, C, H, W)$ to a noise prediction of the same shape, while also conditioning on the scalar timestep $t$. The U-Net architecture is the standard choice due to its ability to process information at multiple scales while preserving spatial dimensionality via skip connections.

For CIFAR-10, the U-Net must be adapted to handle $32 \times 32$ resolution. The architecture is significantly deeper and wider than standard classifiers to capture the fine-grained probabilistic details required for generation.

### 3.1 U-Net Components and Configuration

The architecture is built from Residual Blocks, Self-Attention mechanisms, and Time Embeddings.

**Table 2: U-Net Hyperparameters for CIFAR-10**

| Parameter            |              Value | Justification                                  |
| -------------------- | -----------------: | ---------------------------------------------- |
| Base Channels        |                128 | Sufficient capacity for complex distributions. |
| Channel Multipliers  | 128, 256, 256, 256 | Channels scale at resolutions 32, 16, 8, 4.    |
| Attention Resolution |     $16 \times 16$ | Capture global context at mid resolution.      |
| Residual Blocks      |        2 per level | Depth per resolution level.                    |
| Dropout              |                0.1 | Regularization to prevent overfitting.         |
| Normalization        |          GroupNorm | Stable for small batch sizes.                  |
| Activation           |       SiLU (Swish) | Smooth activation aids optimization.           |

### 3.2 Time Embedding Injection

The timestep $t$ is embedded using sinusoidal positional embeddings (as in Transformers) and then passed through an MLP.

$$
\begin{aligned}
PE_{(t, 2i)} &= \sin!\left(t / 10000^{2i/d_{\text{model}}}\right), \
PE_{(t, 2i+1)} &= \cos!\left(t / 10000^{2i/d_{\text{model}}}\right).
\end{aligned}
$$

In each Residual Block, the time embedding is projected to match the block’s channel dimension and added to feature maps, allowing the network to adapt to different noise levels.

### 3.3 The Wide ResNet Block

The fundamental unit is the Wide ResNet block with GroupNorm and time embedding.

**Forward pass structure:**

1. Input $x$ $\rightarrow$ GroupNorm $\rightarrow$ SiLU $\rightarrow$ Conv $3 \times 3$ $\rightarrow$ $h$
2. Time embedding $t_{\text{emb}}$ $\rightarrow$ Linear projection $\rightarrow$ add to $h$
3. $h$ $\rightarrow$ GroupNorm $\rightarrow$ SiLU $\rightarrow$ Dropout (0.1) $\rightarrow$ Conv $3 \times 3$ $\rightarrow$ Output
4. Skip connection: add input $x$ (project via $1 \times 1$ conv if channels differ) to Output

---

## 4. PyTorch Implementation Tutorial

This section translates the theoretical design into a modular PyTorch implementation.

### 4.1 Prerequisites and Setup

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
import torchvision
import torchvision.transforms as transforms
from tqdm import tqdm
import math

# Configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
```

### 4.2 Data Preparation

Diffusion models operate in the pixel range $[-1, 1]$.

```python
def get_dataloader(batch_size=128, num_workers=4):
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
    ])

    dataset = torchvision.datasets.CIFAR10(
        root="./data",
        train=True,
        download=True,
        transform=transform,
    )

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        drop_last=True,
    )
    return dataloader
```

### 4.3 U-Net Submodules

```python
class TimeEmbedding(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, t):
        device = t.device
        half_dim = self.dim // 2
        embeddings = math.log(10000) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)
        embeddings = t[:, None] * embeddings[None, :]
        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)
        return embeddings


class ResidualBlock(nn.Module):
    def __init__(self, in_channels, out_channels, time_channels, dropout=0.1):
        super().__init__()
        self.norm1 = nn.GroupNorm(32, in_channels)
        self.act1 = nn.SiLU()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)

        self.time_emb = nn.Sequential(
            nn.SiLU(),
            nn.Linear(time_channels, out_channels),
        )

        self.norm2 = nn.GroupNorm(32, out_channels)
        self.act2 = nn.SiLU()
        self.dropout = nn.Dropout(dropout)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)

        if in_channels != out_channels:
            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1)
        else:
            self.shortcut = nn.Identity()

    def forward(self, x, t):
        h = self.conv1(self.act1(self.norm1(x)))
        h += self.time_emb(t)[:, :, None, None]
        h = self.conv2(self.dropout(self.act2(self.norm2(h))))
        return h + self.shortcut(x)


class SelfAttention(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)
        self.ln = nn.LayerNorm([channels])
        self.ff_self = nn.Sequential(
            nn.LayerNorm([channels]),
            nn.Linear(channels, channels),
            nn.GELU(),
            nn.Linear(channels, channels),
        )

    def forward(self, x):
        B, C, H, W = x.shape
        x = x.view(B, C, H * W).swapaxes(1, 2)
        x_ln = self.ln(x)
        attn_value, _ = self.mha(x_ln, x_ln, x_ln)
        attn_value = attn_value + x
        attn_value = self.ff_self(attn_value) + attn_value
        return attn_value.swapaxes(1, 2).view(B, C, H, W)
```

### 4.4 The Full U-Net Architecture

```python
class UNet(nn.Module):
    def __init__(self, c_in=3, c_out=3, time_dim=256, base_channels=128):
        super().__init__()

        self.time_mlp = nn.Sequential(
            TimeEmbedding(time_dim),
            nn.Linear(time_dim, time_dim),
            nn.SiLU(),
            nn.Linear(time_dim, time_dim),
        )

        self.inc = nn.Conv2d(c_in, base_channels, kernel_size=3, padding=1)

        self.down1 = nn.Sequential(
            ResidualBlock(base_channels, 128, time_dim),
            ResidualBlock(128, 128, time_dim),
        )
        self.down2 = nn.Sequential(
            ResidualBlock(128, 256, time_dim),
            ResidualBlock(256, 256, time_dim),
        )
        self.down3 = nn.Sequential(
            ResidualBlock(256, 256, time_dim),
            ResidualBlock(256, 256, time_dim),
        )

        self.attn_down2 = SelfAttention(256)

        self.down_op1 = nn.Conv2d(128, 128, 3, 2, 1)  # 32 -> 16
        self.down_op2 = nn.Conv2d(256, 256, 3, 2, 1)  # 16 -> 8
        self.down_op3 = nn.Conv2d(256, 256, 3, 2, 1)  # 8 -> 4

        self.bot1 = ResidualBlock(256, 512, time_dim)
        self.bot2 = ResidualBlock(512, 512, time_dim)
        self.bot3 = ResidualBlock(512, 256, time_dim)

        self.up_op1 = nn.ConvTranspose2d(256, 256, 4, 2, 1)  # 4 -> 8
        self.up_op2 = nn.ConvTranspose2d(256, 256, 4, 2, 1)  # 8 -> 16
        self.up_op3 = nn.ConvTranspose2d(128, 128, 4, 2, 1)  # 16 -> 32

        self.up3 = nn.Sequential(
            ResidualBlock(256 + 256, 256, time_dim),
            ResidualBlock(256, 256, time_dim),
        )
        self.up2 = nn.Sequential(
            ResidualBlock(256 + 256, 256, time_dim),
            ResidualBlock(256, 128, time_dim),
        )
        self.attn_up2 = SelfAttention(128)

        self.up1 = nn.Sequential(
            ResidualBlock(128 + 128, 128, time_dim),
            ResidualBlock(128, 128, time_dim),
        )

        self.outc = nn.Conv2d(128, c_out, kernel_size=1)

    def forward(self, x, t):
        t = self.time_mlp(t)

        x1 = self.inc(x)

        x2 = self.down1(x1)
        x2_down = self.down_op1(x2)

        x3 = self.down2(x2_down)
        x3 = self.attn_down2(x3)
        x3_down = self.down_op2(x3)

        x4 = self.down3(x3_down)
        x4_down = self.down_op3(x4)

        x4_down = self.bot1(x4_down, t)
        x4_down = self.bot2(x4_down, t)
        x4_down = self.bot3(x4_down, t)

        x_up3 = self.up_op1(x4_down)
        x_up3 = torch.cat([x_up3, x4], dim=1)
        x_up3 = self.up3(x_up3, t)

        x_up2 = self.up_op2(x_up3)
        x_up2 = torch.cat([x_up2, x3], dim=1)
        x_up2 = self.up2(x_up2, t)
        x_up2 = self.attn_up2(x_up2)

        x_up1 = self.up_op3(x_up2)
        x_up1 = torch.cat([x_up1, x2], dim=1)
        x_up1 = self.up1(x_up1, t)

        return self.outc(x_up1)
```

### 4.5 The Diffusion Utility Class

```python
class Diffusion:
    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=32, device="cuda"):
        self.noise_steps = noise_steps
        self.beta_start = beta_start
        self.beta_end = beta_end
        self.img_size = img_size
        self.device = device

        self.beta = self.prepare_noise_schedule().to(device)
        self.alpha = 1.0 - self.beta
        self.alpha_hat = torch.cumprod(self.alpha, dim=0)

    def prepare_noise_schedule(self):
        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)

    def noise_images(self, x, t):
        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]
        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]
        epsilon = torch.randn_like(x)
        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * epsilon, epsilon

    def sample_timesteps(self, n):
        return torch.randint(low=1, high=self.noise_steps, size=(n,), device=self.device)

    @torch.no_grad()
    def sample(self, model, n):
        model.eval()
        x = torch.randn((n, 3, self.img_size, self.img_size), device=self.device)

        for i in tqdm(reversed(range(1, self.noise_steps)), position=0):
            t = torch.full((n,), i, device=self.device, dtype=torch.long)
            predicted_noise = model(x, t)

            alpha = self.alpha[t][:, None, None, None]
            alpha_hat = self.alpha_hat[t][:, None, None, None]
            beta = self.beta[t][:, None, None, None]

            noise = torch.randn_like(x) if i > 1 else torch.zeros_like(x)

            x = (1 / torch.sqrt(alpha)) * (
                x - ((1 - alpha) / torch.sqrt(1 - alpha_hat)) * predicted_noise
            ) + torch.sqrt(beta) * noise

        model.train()
        x = (x.clamp(-1, 1) + 1) / 2
        x = (x * 255).to(torch.uint8)
        return x
```

---

## 5. Training Dynamics and Monitoring

Training a diffusion model requires patience and precise hyperparameter tuning. Unlike classifiers, where validation accuracy provides a clear signal, the diffusion loss (MSE) is not always indicative of visual sample quality.

### 5.1 Hyperparameter Configuration

**Table 3: Training Hyperparameters**

| Parameter     |              Value | Notes                          |
| ------------- | -----------------: | ------------------------------ |
| Epochs        |            300–500 | 300 is a minimum for good FID. |
| Batch Size    |                128 | Balance VRAM and stability.    |
| Learning Rate | $2 \times 10^{-4}$ | Standard for AdamW.            |
| Weight Decay  | $1 \times 10^{-4}$ | Regularization.                |
| EMA Decay     |             0.9999 | Critical for best samples.     |

### 5.2 Exponential Moving Average (EMA)

A critical component is the Exponential Moving Average (EMA) of model weights:

$$
w_{\text{ema}} = \beta, w_{\text{ema}} + (1-\beta), w_{\text{model}}.
$$

Maintaining a slow-moving shadow copy stabilizes sampling and improves FID.

### 5.3 Training Loop Implementation

```python
import copy
import os

def save_images(images, path):
    grid = torchvision.utils.make_grid(images, nrow=4)
    torchvision.utils.save_image(grid.float() / 255.0, path)

def train_cifar10():
    dataloader = get_dataloader(batch_size=128)
    model = UNet().to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)
    mse = nn.MSELoss()
    diffusion = Diffusion(img_size=32, device=device)

    ema_model = copy.deepcopy(model).eval().requires_grad_(False)

    epochs = 300
    ema_decay = 0.9999

    for epoch in range(epochs):
        pbar = tqdm(dataloader)
        for images, _ in pbar:
            images = images.to(device)

            t = diffusion.sample_timesteps(images.shape[0])
            x_t, noise = diffusion.noise_images(images, t)

            predicted_noise = model(x_t, t)
            loss = mse(noise, predicted_noise)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            for p, p_ema in zip(model.parameters(), ema_model.parameters()):
                p_ema.data.mul_(ema_decay).add_(p.data, alpha=1 - ema_decay)

            pbar.set_postfix(MSE=loss.item())

        if (epoch + 1) % 50 == 0:
            os.makedirs("models", exist_ok=True)
            os.makedirs("results", exist_ok=True)

            torch.save(model.state_dict(), f"models/ddpm_cifar10_ep{epoch+1}.pth")
            torch.save(ema_model.state_dict(), f"models/ddpm_ema_cifar10_ep{epoch+1}.pth")

            sampled_images = diffusion.sample(ema_model, n=16)
            save_images(sampled_images, os.path.join("results", f"{epoch+1}.jpg"))
```

---

## 6. Investigation and Evaluation Framework

A robust evaluation strategy validates the model beyond subjective inspection, using classifier confidence and standard metrics like Inception Score (IS) and Fréchet Inception Distance (FID).

### 6.1 Investigating Generated Images with a Classifier

Protocol:

1. Load checkpoints at epochs 50, 150, 300.
2. Generate 1000 images per checkpoint.
3. Classify images with a pre-trained CIFAR-10 model.
4. Compute mean max confidence and class distribution.

```python
def investigate_with_classifier(generated_images_path):
    from cifar10_models.resnet import resnet18
    classifier = resnet18(pretrained=True).to(device)
    classifier.eval()

    transform = transforms.Compose([
        transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],
                             std=[0.2023, 0.1994, 0.2010])
    ])

    # ... Load images into gen_loader ...
    confidences = []
    class_dist = torch.zeros(10)

    with torch.no_grad():
        for imgs in gen_loader:
            imgs = transform(imgs).to(device)
            outputs = classifier(imgs)
            probs = F.softmax(outputs, dim=1)
            max_prob, preds = torch.max(probs, dim=1)

            confidences.extend(max_prob.cpu().numpy())
            for p in preds:
                class_dist[p] += 1

    avg_confidence = sum(confidences) / len(confidences)
    print(f"Average Classifier Confidence: {avg_confidence:.4f}")
    print(f"Class Distribution: {class_dist}")

    return avg_confidence
```

### 6.2 Calculating Inception Score (IS) and FID

Using `torch-fidelity`:

```bash
fidelity --gpu 0 --fid --isc --input1 ./generated_samples --input2 cifar10-train
```

Python:

```python
import torch_fidelity

def calculate_metrics(gen_folder_path):
    metrics_dict = torch_fidelity.calculate_metrics(
        input1=gen_folder_path,
        input2="cifar10-train",
        cuda=True,
        isc=True,
        fid=True,
        verbose=False,
    )
    print(f"Inception Score: {metrics_dict['inception_score_mean']:.3f}")
    print(f"FID: {metrics_dict['frechet_inception_distance']:.3f}")
```

### 6.3 Progressive Investigation at Training Points

**Table 4: Expected Metric Evolution**

| Training Point | Visual Characteristics                          | Avg Classifier Confidence | Estimated FID |
| -------------- | ----------------------------------------------- | ------------------------: | ------------: |
| Epoch 10       | Blurry blobs, rough global structure            |                     ~0.20 |         > 150 |
| Epoch 100      | Recognizable shapes, grainy textures            |                     ~0.65 |        ~30–50 |
| Epoch 300      | Sharp edges, clear objects, diverse backgrounds |                    ~0.85+ |          < 10 |

---

## 7. Sampling Algorithms: Ancestral vs. DDIM

Standard ancestral sampling (stochastic):

$$
x_{t-1} =
\frac{1}{\sqrt{\alpha_t}}
\left(
x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}*t}},\epsilon*\theta(x_t, t)
\right)

* \sigma_t z.
  $$

DDIM accelerates sampling by making the reverse process deterministic and allowing step skipping (e.g., 50 steps instead of 1000). In production, DDIM is often preferred for speed; DDPM sampling can yield more stochastic diversity.

---

## 8. Conclusion

This report detailed an end-to-end DDPM implementation for CIFAR-10 in PyTorch. By combining a theoretically grounded loss with a tuned U-Net (Wide ResNet blocks, attention, GroupNorm) and robust training practices (EMA, linear schedule), it is possible to achieve high-fidelity image synthesis.

The evaluation protocol—classifier-based semantic checks plus standardized metrics like FID via `torch-fidelity`—provides a comprehensive view of model performance. While diffusion models are computationally intensive during training and sampling, they provide a stable and scalable alternative to previous generative paradigms.
